#!/usr/bin/python3

# OpenBACH is a generic testbed able to control/configure multiple
# network/physical entities (under test) and collect data from them. It is
# composed of an Auditorium (HMIs), a Controller, a Collector and multiple
# Agents (one for each network entity that wants to be tested).
#
#
# Copyright Â© 2016-2023 CNES
#
#
# This file is part of the OpenBACH testbed.
#
#
# OpenBACH is a free software : you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY, without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#
# You should have received a copy of the GNU General Public License along with
# this program. If not, see http://www.gnu.org/licenses/.


"""RStats daemon

This module defines the necessary building blocks to receive, tag and
route the various statistics generated by the jobs part of the OpenBACH
testbed.
"""


__author__ = 'Viveris Technologies'
__credits__ = '''Contributors:
 * Adrien THIBAUD <adrien.thibaud@toulouse.viveris.com>
 * Mathias ETTINGER <mathias.ettinger@toulouse.viveris.com>
 * Joaquin MUGUERZA <joaquin.muguerza@toulouse.viveris.com>
'''


import socket
import syslog
import os.path
import logging
import functools
import threading
import traceback
import contextlib
import configparser
import socketserver
from itertools import groupby
from time import strftime
from datetime import datetime
from collections import namedtuple
try:
    import simplejson as json
except ImportError:
    import json

import yaml


DEFAULT_LOG_PATH = '/var/openbach_stats/'
RSTATS_CONFIG_FILE = '/opt/openbach/agent/rstats/rstats.yml'
COLLECTOR_CONFIG_FILE = '/opt/openbach/agent/collector.yml'


class BadRequest(ValueError):
    """Base exception for this module"""
    def __init__(self, reason):
        super().__init__(reason)
        self.reason = reason


class AutoClosingFileHandler(logging.FileHandler):
    def __init__(self, filename, encoding=None):
        """
        Delay appending to the specified file and use it as the stream for logging.
        """
        super().__init__(filename, 'a', encoding, True)

    def emit(self, record):
        """
        Emit a record.

        Uses the delay mechanic of the parent class to open the
        proper stream before emitting.

        Closes the stream right after.
        """
        super().emit(record)
        stream = self.setStream(None)
        if stream and hasattr(stream, 'close'):
            stream.close()


@functools.lru_cache(maxsize=1)
def get_statistics_sender():
    """Build the function that will route data to the logstash
    server based on the provided configuration files.
    """

    with open(COLLECTOR_CONFIG_FILE, encoding='utf-8') as stream:
        content = yaml.safe_load(stream)
    host = content['address']
    port = content['stats']['port']
    address = (host, int(port))

    # Build functions to send stats to the configured address

    @contextlib.contextmanager
    def socket_error_to_bad_request(message):
        """Helper context manager aimed at reducing boilerplate code"""
        try:
            yield
        except socket.error as err:
            raise BadRequest(message.format(*err))

    def send_udp(data):
        with socket_error_to_bad_request('Failed to create socket'):
            logstash = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

        with logstash:
            with socket_error_to_bad_request('Error code: {}, Message {}'):
                logstash.sendto(data.encode(), address)

    def send_tcp(data):
        with socket_error_to_bad_request('Failed to create socket'):
            logstash = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        with logstash:
            with socket_error_to_bad_request('Failed to connect to server'):
                logstash.connect(address)
            with socket_error_to_bad_request('Error code: {}, Message {}'):
                logstash.send(data.encode())

    with open(RSTATS_CONFIG_FILE, encoding='utf-8') as stream:
        content = yaml.safe_load(stream)

    # Select the right function to use based on the configured mode
    try:
        return {
            'tcp': send_tcp,
            'udp': send_udp,
        }[content['logstash']['mode']]
    except KeyError:
        raise BadRequest('Mode not known')


class Rstats:
    def __init__(self, connection_id, logpath=DEFAULT_LOG_PATH, confpath='',
                 suffix=None, job_name=None, job_instance_id=0,
                 scenario_instance_id=0, owner_scenario_instance_id=0,
                 agent_name='agent_name_not_found', reset_handlers=False):
        self._mutex = threading.Lock()

        # We do no want to locally store the files again if the admin
        # job send_stats retransmits the stats of a given job
        if job_name.split('-')[0] == 'send_stats':
            job_name = job_name.split('-')[1]
            store_local = False
            reset_handlers = True
        else:
            store_local = True
        
        self.metadata = {
                'job_name': 'rstats' if job_name is None else job_name,
                'agent_name': agent_name,
                'job_instance_id': job_instance_id,
                'scenario_instance_id': scenario_instance_id,
                'owner_scenario_instance_id': owner_scenario_instance_id,
        }
        
        if suffix is not None:
            self.metadata['suffix'] = suffix

        self._logger = logging.getLogger('rstats-{}'.format(connection_id))

        # Reset the handlers if they store logs for another job
        if not reset_handlers:
            for handler in self._logger.handlers:
                if not isinstance(handler, logging.FileHandler):
                    continue
                handler_job = os.path.basename(os.path.dirname(handler.baseFilename))
                if handler_job != job_name:
                    reset_handlers = True
                    break

        self._confpath = confpath
        self.reload_conf(reset_handlers, store_local, logpath)

    def reload_conf(self, reset_handlers=False, store_local=True, logpath=DEFAULT_LOG_PATH):
        config = configparser.ConfigParser()
        with self._mutex:
            self._rules = {'default': RstatsRule(
                    'default',
                    RstatsRule.ACCEPT,
                    RstatsRule.ACCEPT,
                    RstatsRule.ACCEPT,
            )}
            try:
                config.read(self._confpath)
            except configparser.Error:
                return

            self._rules.update(
                    (name, RstatsRule(
                        name,
                        section.getboolean('local', RstatsRule.ACCEPT),
                        section.getboolean('storage', RstatsRule.ACCEPT),
                        section.getboolean('broadcast', RstatsRule.ACCEPT),
                    ))
                    for name, section in config.items()
                    if section.values()
            )

        if reset_handlers:
            for handler in self._logger.handlers:
                self._logger.removeHandler(handler)

        if store_local and any(rule.local for rule in self._rules.values()):
            if not self._logger.hasHandlers():
                self._logger.setLevel(logging.INFO)
                filename = '{}_{}.stats'.format(self.metadata['job_name'], strftime("%Y-%m-%dT%H%M%S"))
                logfile = os.path.join(logpath, self.metadata['job_name'], filename)
                try:
                    fhd = AutoClosingFileHandler(logfile)
                except OSError:
                    pass
                else:
                    fhd.setFormatter(logging.Formatter('{message}', style='{'))
                    self._logger.addHandler(fhd)
        else:
            for handler in self._logger.handlers:
                self._logger.removeHandler(handler)

    def send_stat(self, suffix, time, stats, files):
        with self._mutex:
            statistics_metadata = {'time': time, 'is_file': files, **self.metadata}
            if suffix is not None:
                statistics_metadata['suffix'] = suffix

            statistics_by_flag = sorted((
                {statistic_name: value}
                for statistic_name, value in stats.items()
            ), key=self._get_flag)

            for flag, statistics_group in groupby(statistics_by_flag, self._get_flag):
                statistics_metadata['flag'] = flag
                statistics = {
                        name: value
                        for statistic in statistics_group
                        for name, value in statistic.items()
                }
                if flag:
                    statistics['_metadata'] = statistics_metadata
                    get_statistics_sender()(json.dumps(statistics))

                # Filter out stats specifically specified local = False or
                # include only those specified local = True, if default is False
                use_local = self._rules['default'].local
                statistics = {
                        name: value
                        for name, value in statistics.items()
                        if (
                            name not in self._rules or self._rules[name].local
                            if use_local else
                            name in self._rules and self._rules[name].local
                        )
                }
                statistics['_metadata'] = statistics_metadata
                if len(statistics) > 1:
                    self._logger.info(json.dumps(statistics))


    def _get_flag(self, statistic_holder):
        statistic_name, = statistic_holder
        if statistic_name in self._rules:
            return self._rules[statistic_name].flag
        else:
            return self._rules['default'].flag


class RstatsRule(namedtuple('RstatsRule', 'name local storage broadcast')):
    ACCEPT = True
    DENY = False

    @property
    def flag(self):
        return bool(self.storage) + 2 * bool(self.broadcast)

    @staticmethod
    def _rule_to_str(rule_value):
        return 'ACCEPT' if rule_value else 'DENY'

    def __str__(self):
        return 'local: {}, storage: {}, broadcast: {} for {}'.format(
                self._rule_to_str(self.local),
                self._rule_to_str(self.storage),
                self._rule_to_str(self.broadcast),
                self.name)


class StatsManager:
    """Borg storing the connections opened with the daemon"""

    __shared_state = {
            'stats': {},
            'cache': {},
            'mutex': threading.RLock(),
            'id': 0,
    }

    def __init__(self):
        self.__dict__ = self.__class__.__shared_state

    def __enter__(self):
        self.mutex.acquire()
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.mutex.release()

    def statistic_lookup(self, instance_id, scenario_id):
        key = (instance_id, scenario_id)
        with self.mutex:
            try:
                return self.cache[key]
            except KeyError:
                self.id += 1
                self.cache[key] = self.id
                return self.id

    @contextlib.contextmanager
    def _id_check(self):
        try:
            yield
        except KeyError:
            raise BadRequest("The given id doesn't represent an open connection")

    def __getitem__(self, id_):
        with self._id_check():
            return self.stats[id_]

    def __setitem__(self, id_, statistic):
        with self.mutex:
            self.stats[id_] = statistic

    def __delitem__(self, id_):
        with self.mutex, self._id_check():
            del self.stats[id_]

    def __iter__(self):
        with self.mutex:
            yield from self.stats.items()

    def reset(self):
        with self.mutex:
            self.stats.clear()
            self.cache.clear()
            self.id = 0


######################################
# Implementation of allowed requests #
######################################

@contextlib.contextmanager
def _handle_parse_errors(name, expected_type):
    try:
        yield
    except ValueError:
        raise BadRequest('Message not formed well. Argument {} should '
                         'be of type {}'.format(name, expected_type))


def create_stat(confpath, job_name, job_instance_id, scenario_instance_id,
                owner_scenario_instance_id, agent_name, override=False):
    # Type conversion
    with _handle_parse_errors('job_instance_id', 'integer'):
        job_instance_id = int(job_instance_id)
    with _handle_parse_errors('scenario_instance_id', 'integer'):
        scenario_instance_id = int(scenario_instance_id)
    with _handle_parse_errors('owner_scenario_instance_id', 'integer'):
        owner_scenario_instance_id = int(owner_scenario_instance_id)
    with _handle_parse_errors('override', 'boolean'):
        override = bool(int(override))

    with StatsManager() as manager:
        statistic_id = manager.statistic_lookup(job_instance_id, scenario_instance_id)

        if override or statistic_id not in manager:
            manager[statistic_id] = Rstats(
                    statistic_id,
                    confpath=confpath,
                    job_name=job_name,
                    job_instance_id=job_instance_id,
                    scenario_instance_id=scenario_instance_id,
                    owner_scenario_instance_id=owner_scenario_instance_id,
                    agent_name=agent_name,
                    reset_handlers=override)

    return statistic_id


def send_stat(connection_id, timestamp, statistics, suffix=None, stored_files=False):
    # Type conversion
    with _handle_parse_errors('connection_id', 'integer'):
        connection_id = int(connection_id)
    with _handle_parse_errors('timestamp', 'integer'):
        timestamp = int(timestamp)
    with _handle_parse_errors('timestamp', 'timestamp in milliseconds'):
        date = datetime.fromtimestamp(timestamp / 1000)
        if date.year == 1970:
            # Most likely a timestamp in seconds, not milliseconds
            raise ValueError

    client_connection = StatsManager()[connection_id]
    client_connection.send_stat(suffix, timestamp, statistics, stored_files)


def reload_stat(connection_id):
    # Type conversion
    with _handle_parse_errors('connection_id', 'integer'):
        connection_id = int(connection_id)

    client_connection = StatsManager()[connection_id]
    client_connection.reload_conf()


def remove_stat(connection_id):
    # Type conversion
    with _handle_parse_errors('connection_id', 'integer'):
        connection_id = int(connection_id)

    del StatsManager()[connection_id]


def reload_stats():
    for _, client_connection in StatsManager():
        client_connection.reload_conf()


def change_config(scenario_instance_id, job_instance_id, enable_broadcast, enable_storage):
    # Type conversion
    with _handle_parse_errors('job_instance_id', 'integer'):
        job_instance_id = int(job_instance_id)
    with _handle_parse_errors('scenario_instance_id', 'integer'):
        scenario_instance_id = int(scenario_instance_id)
    with _handle_parse_errors('enable_broadcast', 'boolean'):
        enable_broadcast = bool(int(enable_broadcast))
    with _handle_parse_errors('enable_storage', 'boolean'):
        enable_storage = bool(int(enable_storage))

    with StatsManager() as manager:
        id = manager.statistic_lookup(job_instance_id, scenario_instance_id)
        client_connection = manager[id]
        default_rule = RstatsRule('default', RstatsRule.ACCEPT, enable_storage, enable_broadcast)
        client_connection._rules['default'] = default_rule


def restart():
    with StatsManager() as manager:
        manager.reset()
        get_statistics_sender.cache_clear()


#####################
# Requests handling #
#####################

class RstatsRequestHandler(socketserver.BaseRequestHandler):
    AVAILABLE_FUNCTIONS = [
            create_stat,
            send_stat,
            reload_stat,
            remove_stat,
            reload_stats,
            change_config,
            restart,
    ]

    def handle(self):
        data, sock = self.request
        msg = 'KO: Unhandled exception occured\0'
        try:
            data = data.decode()
            syslog.syslog(syslog.LOG_INFO, data)
            result = self.execute_request(data)
        except BadRequest as e:
            syslog.syslog(syslog.LOG_ERR, traceback.format_exc())
            msg = 'KO: {}\0'.format(e.reason)
        except Exception as e:
            syslog.syslog(syslog.LOG_CRIT, traceback.format_exc())
            msg = 'KO: An error occured: {}\0'.format(e)
        else:
            if result is None:
                msg = 'OK\0'
            else:
                msg = 'OK {}\0'.format(result)
        finally:
            syslog.syslog(syslog.LOG_INFO, msg)
            sock.sendto(msg.encode(), self.client_address)

    def execute_request(self, data):
        try:
            command = json.loads(data)
        except json.JSONDecodeError:
            raise BadRequest('Request is not a valid JSON string')

        try:
            request = command['command_id']
            args = command['command_parameters']
        except KeyError as e:
            raise BadRequest('Request is missing parameters \'{}\''.format(e))

        try:
            # Compensate for collect_agent using 1-based indexing
            function = self.AVAILABLE_FUNCTIONS[request - 1]
        except (TypeError, IndexError):
            raise BadRequest('Type of request not recognized')

        try:
            return function(**args)
        except TypeError as e:
            raise BadRequest('Arguments mismatch: {}'.format(e))


class RstatsServer(socketserver.ThreadingMixIn, socketserver.UDPServer):
    allow_reuse_address = True
    max_packet_size = 2**14


if __name__ == '__main__':
    syslog.openlog('openbach_rstats', syslog.LOG_PID, syslog.LOG_USER)
    server = RstatsServer(('', 1111), RstatsRequestHandler)
    try:
        server.serve_forever()
    finally:
        server.server_close()
